---
title: Meeting Tesi 2021-03-22
tags: meeting
---
<style>.markdown-body { max-width: 1500px; }</style>

:::info
**Meeting Tesi 2021-03-22**

- **Location:** Skype
- **Date:** 2021-03-22
- **Agenda**
[TOC]
- **Participants:**
    - Alessandro Margara (AM)
    - Gianpaolo Cugola (GC)
    - Marco Donadoni (MD)
    - Edoardo Morassutto (EM)

:::

## Reviewer notes

> D1. First, I would suggest conducting a qualitative and experimental comparison to Timely dataflow. This might in fact prove hard to produce publishable results, given that Timely is a quite mature system with features such as shared indexes, backpressure, and nested iterations, currently missing from RStream.

Not sure if backpressure is handled for real.

> D2. It was unclear to me whether RStream proposes any system innovation or alternative design compared to existing stream processing systems. Is it a clone of Flink having as its sole purpose to shed light into the overheads stemming from the choice of a JVM-based language or are there other aspects that make RStream unique? Answering the question of how much the choice of language vs. other design choices contribute to the performance gap would be challenging and you might want to replicate certain important aspects, such as scheduling, progress tracking, and load management.


> D3. Clarify the iteration semantics of RStream (Section 3.2). As far as I understand, the “iterate” operator provides window iterations on a fixed input and is stateless, while the “cycle” operator is stateful but requires gathering all updates at a single task. This is both a synchronization point and a scalability bottleneck as it assumes state will always fit in the memory of a single task. The Naiad paper describes an elegant way to implement flexible iterations without these limitations.

TODO: investigate the Naiad thing

> D4. Please specify how event time and progress tracking are implemented in RStream and especially how watermarks interact with iterative computations. One reason why systems like Flink and Spark Streaming don’t offer general-purpose iterations is that the watermark mechanism might lead to deadlocks in cyclic dataflows.

RStream misses progress tracking, a source emits watermarks but has no idea on where those watermarks have arrived.

Why deadlocks?

> D5. Please elaborate on RStream’s state management and fault-tolerance capabilities. It would be unfair to claim victory against systems like Flink which provide managed state and exactly-once fault-tolerance if RStream operates on in-memory state only.

Fair point. We should remember to double-check on how to disable those features.

> D6. There exist various streaming benchmarks that you could use for a more convincing evaluation. For example, see the Nexmark benchmark [6] and the Streamline benchmark [7]. These benchmarks provide streaming data sources that generate events continuously and can more faithfully simulate the workload of a stream processing application compared to reading from files.

Yes.

-----

> Lack of insight clear motivation whether garbage-collected language runtime is the problem


> Unclear separation of language (garbage collected JVM-based) vs. implementation (fault tolerance, the use of "standard" libraries)


> Unfinished implementation with possibly unfair comparison - both Apache Flink and Spark invest lots of effort into fault tolerance, restart, and monitoring/visibility (e.g., profiling, DAG/task visualization) that is lacking in the RStream system.

> What leads you to believe that GC is the main reason for inefficiency? Can you provide some empirical evidence?

---

> No fault-tolerance support. The paper acknowledges this gap in the last paragraph, handwaving that this can be handled in future work. I expected such a discussion to be part of the design discussion. It may be that once support is added, a large fraction of the benefit disappears.

> No scheduler, so no support for the placement of operators that is informed by the network topology or the expected data exchange between operators





## [Timely Dataflow](https://timelydataflow.github.io/timely-dataflow/introduction.html): main differences


- All the graph definition is inside a closure: much easier to spawn threads
- Each operator is inside its own "block"
- Computation is advanced calling `.step()`
- Cooperative multitasking:
    - Operators of each block are executed one after the other inside the same thread, non-preemptively
    - Each operator is replicated as many times as threads
    - Data inside the same thread use `VecDeque` and not channels to send data
    - The operator may produce some output and this marks its successors as _schedulable_ and they will be immediately scheduled inside the same step
- It avoid everywhere possible to move memory:
    - Share data and use `std::mem::swap`
    - Avoid copying and dropping almost everything
- Capabilities:
    - If an operator wants to send a message with a given timestamp, it has to have the proper capability
    - Dropping (or downgrading) a capability is equivalent of sending a watermark: the operator will never be able to send any message with timestamp less or equal to that capability (without having other capabilities stored)
    - Capabilities are not optional
    - Operators have an initial capability (i.e. timestamp zero), and a capability for each incoming message
        - The initial one is used to send messages before receiving any
    - Batches are tagged with timestamps, not single messages
- Writing custom operators is easy, the ones provided are very few
    - There aren't windows of any sort
    - There isn't the reduce operator (or any aggregation operator)
- Special _probe_ operators can be used to track the progress of the computation signaling when a "watermark" passes
- Messages are serialized with either `bincode` or `abomonation` (defaults to `abomonation`)

Notes from [the paper](https://dl.acm.org/doi/pdf/10.1145/2517349.2522738) to fact-check from the code:

- Nagle's algorithm is disabled reducing the latency for small messages (usually present in the last steps of the computation)
    - True
- Fault tolerance is implemented
    - False [#102](https://github.com/TimelyDataflow/timely-dataflow/issues/102)

## Proposed benchmarks by paper reviewers
- Nexmark
    - From Apache Beam
    - [Home](https://beam.apache.org/documentation/sdks/java/testing/nexmark/#what-it-is) / [old reference](https://web.archive.org/web/20100620010601/http://datalab.cs.pdx.edu/niagaraST/NEXMark/)
    - All kinds of queries, from simple maps to complex joins on windows
- Streamline
    - [Paper](https://arxiv.org/pdf/1802.08496.pdf)
    - Already considered

## Early benchmarks

### Wordcount

Tested on `gutenberg10.txt` (10x `gutenberg.txt`).

Values in parenthesis are speedup w.r.t. single core.

```csvpreview {header="true"}
th,hosts,network,timely-dataflow,differential-dataflow,rstream1,rstream2 master,rstream2 sync,rstream2 sync++,LB
,,,non-assoc,assoc?,non-assoc,non-assoc¹,non-assoc,non-assoc²,non-assoc

1,sola3,,53.0s (1),67.2s (1),65.2s (1),127.8s (1),147.9s (1),96.0s (1),56.7s (1)
2,sola3,,32.4s (1.6),39.9s (1.68),38.4s (1.69),78.5s (1.62),100.6s (1.47),45.6s (2.11),29.4s (1.92)
4,sola3,,17.7s (2.99),23s (2.92),24.3s (2.68),44.3s (2.88),64.6s (2.29),25.3s (3.79),15.7s (3.61)
8,sola3,,13.4s (3.96),18.1s (3.71),19.6s (3.33),31.1s (4.1),85.0 (1.74),21.3s (4.51),13.1s (4.32)

2,sola1/3,1gb,32s (2),44.3s (2),46.4s (2),71.0s (2),89.8s (2),40.2s (2)
4,sola1/3,1gb,27.3s (2.34),42.9s (2.07),31.7s (2.93),42.2s (3.36),59s (3.04),21.1s (3.81)
8,sola1/3,1gb,27.7s (2.31),43.4s (2.04),32.9s (2.82),21.3s (6.67),39.3s (4.57),13.2s (6.09)
12,sola1/3,1gb,28.2s (2.26),44.1s (2.01),32.5s (2.85),17.6s (8.07),40s (4.49),11.7s (6.87)
16,sola1/3,1gb,30.2s (2.11),48.8s (1.81),32.8s (2.83),17.3s (8.21),42.8s (4.20),11.1s (7.24)

2,sola1/3,10gb,31.7s (2),40.9s (2),33.8s (2),70.7s (2),85.4s (2),38.2s (2)
4,sola1/3,10gb,17.7s (3.58),22.7s (3.6),19.1s (3.54),41.4s (3.42),57.5s (2.97),20.2s (3.78)
8,sola1/3,10gb,9.9s (6.4),11.8s (6.93),12.0s (5.63),21.9s (6.46),39.2s (4.36),12.9s (5.92)
12,sola1/3,10gb,9.8s (6.47),11.4s (7.18),10.1s (6.69),17.5s (8.08),36.7s (4.65),11.2s (6.88)
16,sola1/3,10gb,8.0s (7.93),10.7s (7.64),9.95s (6.79),16.8s (8.41),40.2s (4.25),10.8s (7.07)
```

¹ it uses all the cores for the async runtime, `th` represents the number of replicas of each block.

² remove batching threads and ignoring adaptive timeout.

:::spoiler `timely-dataflow` wordcount source
```rust=
worker.dataflow::<usize, _, _>(|scope| {
    input
        .to_stream(scope)
        .flat_map(move |line: String| tokenizer.tokenize(line))
        .unary_frontier(exchange, "mapper", |_capability, _info| {
            let mut counts = HashMap::new();
            let mut capability = None;

            move |input, output| {
                while let Some((time, data)) = input.next() {
                    if capability.is_none() {
                        capability = Some(time.retain());
                    }

                    for word in data.replace(Vec::new()).into_iter() {
                        counts.entry(word).and_modify(|c| *c += 1).or_insert(1);
                    }
                }

                if let Some(c) = &capability {
                    if !input.frontier().less_equal(c.time()) {
                        let mut session = output.session(c);
                        for e in counts.drain() {
                            session.give(e)
                        }
                        capability = None;
                    }
                }
            }
        })
        .unary_frontier(Exchange::new(|_| 0), "reducer", |_capability, _info| {
            let mut count = 0;
            let mut capability = None;
            move |input, output| {
                while let Some((time, data)) = input.next() {
                    if capability.is_none() {
                        capability = Some(time.retain());
                    }
                    count += data.len();
                }
                if let Some(c) = &capability {
                    if !input.frontier().less_equal(c.time()) {
                        let mut session = output.session(c);
                        session.give(count);
                        capability = None;
                    }
                }
            }
        })
        .inspect(|x| println!("Total: {} words", x))
        .probe_with(&mut probe);
});
```
:::


:::spoiler `differential-dataflow` wordcount source
```rust=
let (mut input, probe) = worker.dataflow::<usize, _, _>(|scope| {
    let (input, lines) = scope.new_collection();
    let probe = lines
        .flat_map(move |line: String| tokenizer.tokenize(line))
        .count_total()
        .map(|_| 0u8)
        .count_total()
        .inspect(|((_, x), _, _)| println!("Total: {} words", x))
        .probe();
    (input, probe)
});
```
:::

:::spoiler `rstream2` wordcount source
```rust=
let stream = env
    .stream(source)
    .flat_map(move |line| tokenizer.tokenize(line))
    .group_by(|word| word.clone())
    .fold(0, |count, _word| count + 1)
    .unkey();
```
:::

## Batching problems

`sync` was slow because of the huge number of threads for the batching:

- they were there for handling the timeout of `Adaptive`
- way too many of them, it's one of the main causes of bottleneck

Possible solutions:

1. Move the `Batcher` inside `NetworkSender`
    - This allows the network thread to have a timeout
1. Each `Batcher` has an extra thread which wakes up only for the timeout
    - Changes the semantics of `Adaptive`
    - Still many many threads
    - Requires mutexes and a lock for each message
1. The source keeps track of the timeout
    - When the source doesn't receive a message within the timeout it emits a _flush_ message
    - When the batcher has not sent a message within a timeout or when it receives a _flush_ message, it sends the data
    - We implemented this one

:::danger
TSC on some AMD is buggy and linux fallbacks to a very slow implementation of `clock_gettime` that impacts significantly on the performances.

On Marco's PC (AMD 4750U, 8 cores, 16 threads) with 2 processes, 4 replica each, on a 5x Gutenberg dataset:

- `BatchMode::Fixed(1024)` → 11.54s
- `BatchMode::Adaptive(1024, 5s)` → 26.3s

Linux fallbacks from `tsc` to `hpet`:
```bash
$ cat /sys/devices/system/clocksource/clocksource0/current_clocksource
hpet
$ cat /sys/devices/system/clocksource/*/available_clocksource
hpet acpi_pm
```

Each call to `clock_gettime` become a syscall that maybe skips vDSO: [source](https://blog.packagecloud.io/eng/2017/03/08/system-calls-are-much-slower-on-ec2/)

A possible solution is to switch to [rust-coarsetime](https://github.com/jedisct1/rust-coarsetime) that reduces precision and uses `clock_gettime(CLOCK_MONOTONIC_COARSE)`. Using `strace` we proved that this call won't perform any syscall and it's orders of magnitude faster.
:::